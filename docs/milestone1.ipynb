{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "The data used for this project is taken from the `brise-plandok` project on [github](https://github.com/recski/brise-plandok). It is already separated in a test, train and validation set. The following python script can also be executed from command line using python `python download.py` in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "csv_urls = [\n",
    "    \"https://raw.githubusercontent.com/recski/brise-plandok/main/brise_plandok/baselines/input/test_data.csv\",\n",
    "    \"https://raw.githubusercontent.com/recski/brise-plandok/main/brise_plandok/baselines/input/train_data.csv\",\n",
    "    \"https://raw.githubusercontent.com/recski/brise-plandok/main/brise_plandok/baselines/input/valid_data.csv\"\n",
    "]\n",
    "for url in csv_urls:\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    set = url.split(\"/\")[-1].split(\"_\")[0]\n",
    "    open(f'../data/{set}_data.csv', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from data.constants import ALL_LABELS_SORTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8020d18a228149e0a44b47f5e988ce3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 01:49:38 INFO: Downloading default packages for language: de (German) ...\n",
      "2022-12-01 01:49:40 INFO: File exists: C:\\Users\\maxha\\stanza_resources\\de\\default.zip\n",
      "2022-12-01 01:49:45 INFO: Finished downloading models and saved to C:\\Users\\maxha\\stanza_resources.\n",
      "2022-12-01 01:49:45 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2456fdb4c384447a6f3fa0ac008e411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 01:49:45 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2022-12-01 01:49:45 INFO: Use device: cpu\n",
      "2022-12-01 01:49:45 INFO: Loading: tokenize\n",
      "2022-12-01 01:49:45 INFO: Loading: mwt\n",
      "2022-12-01 01:49:45 INFO: Loading: lemma\n",
      "2022-12-01 01:49:45 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "german_stop_words = nltk.corpus.stopwords.words('german')\n",
    "stanza.download('de')\n",
    "nlp=stanza.Pipeline(processors=\"tokenize,mwt,lemma\", lang=\"de\",use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Path to csv files\n",
    "WD = '../'\n",
    "data_path = WD + 'data/'\n",
    "train_df = pd.read_csv(data_path + 'train_data.csv')\n",
    "valid_df = pd.read_csv(data_path + 'valid_data.csv')\n",
    "test_df = pd.read_csv(data_path + 'test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(input: str):\n",
    "    doc = nlp(input)\n",
    "    return [word.lemma  for sent in doc.sentences for word in sent.words if word.lemma not in german_stop_words]\n",
    "\n",
    "def preprocess_features(df: pd.DataFrame, type_of_data: str, vocab=None):\n",
    "    vectorizer = CountVectorizer(tokenizer=tokenize, vocabulary=vocab, lowercase=False)\n",
    "    features = df.iloc[:, [0, 1]].copy()\n",
    "    vectors = vectorizer.fit_transform(features.Text).toarray()\n",
    "    transformed_features = pd.DataFrame(vectors, columns=vectorizer.get_feature_names_out())\n",
    "    features=features.join(transformed_features)\n",
    "    features.to_csv((data_path+type_of_data + '_features.csv'), index=False)\n",
    "    return features.columns[2:].tolist()\n",
    "\n",
    "def preprocess_labels(df: pd.DataFrame, type_of_data: str):\n",
    "    labels =  df.iloc[:, [0, 2]].copy()\n",
    "    labels.Labels = labels.Labels.apply(ast.literal_eval)\n",
    "    mlb = MultiLabelBinarizer(classes=list(ALL_LABELS_SORTED.keys()))\n",
    "    labels_transformed = mlb.fit_transform(labels['Labels'])\n",
    "    labels[mlb.classes_] = labels_transformed\n",
    "    labels.to_csv((data_path + type_of_data + '_labels.csv'), index=False)\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame, type_of_data: str, vocab=None):\n",
    "    vocab = preprocess_features(df, type_of_data, vocab)\n",
    "    preprocess_labels(df, type_of_data)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run preprocessing\n",
    "\n",
    "The following code extracts features and labels as new csv files from the original files `test_data.csv`, `train_data.csv` and `valid_data.csv`. The newly constructed csv files will be used for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "vocab = preprocess_df(train_df, 'train')\n",
    "preprocess_df(valid_df, 'valid', vocab)\n",
    "vocab=preprocess_df(test_df, 'test', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For this exercise, we chose to train a random forest classifier and a multi-layer perceptron classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team first settled for a random forest classifier, the choice has been driven by the ease of training and RF capacity of handling categories or categorical variables in general. During the project we decided on comparing its performance with a multi-layer perceptron classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_features = pd.read_csv(data_path + 'train_features.csv')\n",
    "df_labels = pd.read_csv(data_path + 'train_labels.csv')\n",
    "valid_features = pd.read_csv(data_path + 'valid_features.csv')\n",
    "valid_labels = pd.read_csv(data_path + 'valid_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train = df_labels.iloc[:, 2:].copy()\n",
    "x_train = df_features.iloc[:, 2:].copy()\n",
    "y_test = valid_labels.iloc[:, 2:].copy()\n",
    "x_test = valid_features.iloc[:, 2:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "classifier_rf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42)\n",
    "classifier_deep = MLPClassifier(random_state=1, max_iter=290) # number of iterations chosen so that optimization converges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier_rf.fit(x_train, y_train)\n",
    "classifier_deep.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred_rf = classifier_rf.predict(x_test)\n",
    "y_pred_deep = classifier_deep.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "report_rf=metrics.classification_report(y_test, y_pred_rf, output_dict=True, zero_division=0)\n",
    "report_deep=metrics.classification_report(y_test, y_pred_deep, output_dict=True, zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for multi-label classification\n",
    "\n",
    "We have unbalanced classes, this is a feature to take into account. Maybe tuning loss functions to care about weights. So, accuracy is not a good metric. Better precision, recall, f1-score etc. For each class, we can compute the usual metrics for binary classification and the confusion matrix. Aggregated metrics like macro, micro, weighted and sampled avg give us a high-level view of how our model is performing.\n",
    "\n",
    "- <b>Macro Average</b>\n",
    "This is simply the average of a metric — precision, recall or f1-score — over all classes.\n",
    "\n",
    "- <b>Micro Average</b>\n",
    "The micro-average of a metric is calculated by considering all the TP, TN, FP and FN for each class, adding them up and then using those to compute the metric’s micro-average\n",
    "\n",
    "- <b>Weighted Average</b>\n",
    "This is simply the average of the metric values for individual classes weighted by the support of that class. The support is how many times a class appeared in y_test.\n",
    "\n",
    "- <b>Samples Average</b>\n",
    "Here, we compute metrics for each sample and then average them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "These are preliminary results, obviously to have a robust model we still need to evaluate through, for example, cross-validation on train+validation and then test it in the test set.\n",
    "From the reports, we can also see that some classes are not represented. More focus on this point is needed. Either to make sure the model can be trained on that, or study a model able to generalize.\n",
    "Both the models can be fine tuned in the hyperparameter space, or even extended (using bagging for RF for example, or more complex networks to be evaluated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "macros=['micro avg','macro avg','weighted avg','samples avg']\n",
    "scores=['precision','recall','f1-score']\n",
    "df = pd.DataFrame(columns=['model','measure','value'])\n",
    "\n",
    "for macro in macros:\n",
    "    for score in scores:\n",
    "        df.loc[len(df)] = ['rf',macro+'-'+score, report_rf[macro][score]]\n",
    "        df.loc[len(df)] = ['deep',macro+'-'+score, report_deep[macro][score]]\n",
    "\n",
    "display(df[df[\"model\"]==\"rf\"]),display(df[df[\"model\"]==\"deep\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table demonstrates the performance of ours classifiers. We can see that - in most cases - the multi-layer perceptron classifier yields better results than the random forest classifier. This property is visualized in the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(x='measure', y='value', hue='model', data=df,\n",
    "            palette='tab10', edgecolor='w')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=14):\n",
    "\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names,)\n",
    "\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    axes.set_ylabel('True label')\n",
    "    axes.set_xlabel('Predicted label')\n",
    "    axes.set_title(class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vis_arr = multilabel_confusion_matrix(y_test, y_pred_rf)\n",
    "labels =  y_test.columns\n",
    "fig, ax = plt.subplots(4, 4, figsize=(16, 7))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "for axes, cfs_matrix, label in zip(ax.flatten(), vis_arr, labels):\n",
    "    print_confusion_matrix(cfs_matrix, axes, label, [\"N\", \"Y\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_arr = multilabel_confusion_matrix(y_test, y_pred_deep)\n",
    "labels =  y_test.columns\n",
    "fig, ax = plt.subplots(4, 4, figsize=(16, 7))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "for axes, cfs_matrix, label in zip(ax.flatten(), vis_arr, labels):\n",
    "    print_confusion_matrix(cfs_matrix, axes, label, [\"N\", \"Y\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis\n",
    "\n",
    "The confusion matrices above show that each class does not have many occurrences in the data set. Thus, the classifier seems to perform quite well when predicting negatives than positives. For every label, the number of FN is equal or smaller than the number of FP except for the label \"Planzeichen\". This label occurred more often in the data.\n",
    "\n",
    "For some labels, the classification can be derived from specific keywords. For example predicting the label \"AnordnungGaertnerischeAugestaltung\" depends on certain keywords like \"begrünen\" or \"gärtnerisch\" which seems obvious. The same applies for the label \"GehsteigBreiteMin\" which highly depends on the occurrence of the word \"Gehsteig\" (which also is quite obvious).\n",
    "\n",
    "One problem of the current feature set is that some features are places (\"ottakring\", \"wagram\", \"erzherzogkarlstraße\", \"oktober\"). Whilst these features might be meaningful for the current data and gives some insights on the training data, they should not be considered for classification tasks. They introduce a bias to the classification that will just add noise in the long run.\n",
    "\n",
    "For our application, both recall and precision are important as we want the classifier to be sensitive enough to not miss any cases but also don't want it to add incorrect annotations that have to be manually checked afterwards. So considering the f1-score, the MLP classifier yields much better results.\n",
    "\n",
    "The difference in the choice of the classifier can be seen when looking at the confusion matrix of the label \"Widmung\". The label has 55 TP cases with the MLP classifier and only 24 with RF. The same applied for the label \"BBAllgemein\".\n",
    "\n",
    "We can see that overall, the classification results are good. They are by far not perfect but we also did not tune hyperparameters or used more sophisticated transformations. Further experiments could be the exclusion of variables that imply unwanted bias and dimensionality reduction. Dimensionality reduction should not be done using PCA, though, because we are dealing with descrete data. NMF, PLSA or PLDA could be used for this task.\n",
    "\n",
    "To sum it up, the MLP classifier yielded better results than the random forest. The classification is good but can probably be improved. Therefore, one would need to tune the hyperparameters and might have to employ methods for feature selection or dimensionality reduction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tuwnlpie]",
   "language": "python",
   "name": "conda-env-tuwnlpie-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
